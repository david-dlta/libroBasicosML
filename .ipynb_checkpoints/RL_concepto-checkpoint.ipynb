{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión lineal simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Regresión Lineal simple es un modelo relativamente simple ymuy utilizado en el Machine Learning. Por ello es un buen punto de partida para iniciarse.\n",
    "\n",
    "En la Regresión Lineal la variable dependiente $y$ sigue una función lineal de uno o más predictors $x_1$, $x_2$, ... , $x_D$ más un error. La observación $n$ tendrá la forma: \n",
    "\n",
    "$$y_n = \\beta_0 + \\beta_1x_{n1} +...+ \\beta_1x_{nD} + \\epsilon_n $$ \n",
    "\n",
    "Donde $\\beta_0$ es el interceptor con el eje de coordenadas, $\\beta_1, ... ,\\beta_D$ los coeficientes de las variables y $\\epsilon$ el error entre el valor real y la funcion lineal de los predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las matemáticas se simplifican cuando usamos vectores para representar nuestreos predictors y coeficientes. $x_n$ y $\\beta$ se definene de la sigueinte manera:\n",
    "\n",
    "$$x_n = (1 x_{n1} ... x_{nD})^T$$\n",
    "$$\\beta = (1 \\beta_1 ... \\beta_D)^T$$\n",
    "\n",
    "Por lo que equivalentemente podemos expresar $y_n$ como:\n",
    "\n",
    "$$y_n = \\beta^Tx_n + \\epsilon_n$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez conocemos la estructura que nuestros datos siguen. La tarea del *Machine Learning* es la de estimar los parámetros $\\beta$. se representan con $\\hat{\\beta}$. Estas estimaciones nos dan valores ajustados para la variable target representada por $\\hat{y}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tarea de la estimación se puede realizar de dos maneras. La primera sería mediante la minimización del coste y la otra mediante la maximización de la probabilidad. En este caso nos vamos a centrar en la primera técnica para estimar $\\hat{y}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *Machine Learning* normalmente se suele seleccionar una función de coste para los modelos. Esta funcion de coste define como de bién, dados unos parámetros, se es capaz de estimar los datos observados. Una vez contamos con dicha funcion podemos modificar los parámetros e iterativamente observar como son nuestras predicciones (buenas o malas). La función de coste más usada para la Regresión Lineal Simple es la media de los errores cuadrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de coste de los mínimos cuadrados representa que el coste de nuestro modelo es proporcional a la suma de lso cuadrados de las diferencias entre los valores reales $y_n$ y los valores ajustados $\\hat{y}_n$, $(y_n - \\hat{y}_n)^2$. La razón de elevar al cuadrado la diferencia es la de perjudicar adicionalmente las predicciones que se alejen en exceso de los valores reales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
